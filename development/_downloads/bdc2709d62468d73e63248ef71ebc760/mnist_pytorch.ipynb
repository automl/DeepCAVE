{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Multi-Layer Perceptron via PyTorch\n\nThis more advanced example incorporates multiple objectives, budgets and statusses to\nshow the strenghts of DeepCAVE's recorder.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from inspect import BoundArguments\nimport os\nfrom re import T\nimport time as t\nimport random\nimport ConfigSpace as CS\nfrom ConfigSpace import ConfigurationSpace\nfrom ConfigSpace.hyperparameters import (\n    UniformFloatHyperparameter,\n    UniformIntegerHyperparameter,\n    CategoricalHyperparameter,\n)\nfrom deepcave import Recorder, Objective\nfrom deepcave.runs import Status\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, random_split\nimport torchvision.transforms as transforms\nfrom torchmetrics import Accuracy\nfrom torchvision.datasets import MNIST\nimport pytorch_lightning as pl\n\n\nNUM_WORKERS = 16\n\n\nclass MNISTModel(pl.LightningModule):\n    def __init__(self, activation=\"relu\", learning_rate=1e-4, dropout_rate=0.1, batch_size=64):\n        super().__init__()\n\n        if activation == \"relu\":\n            self.activation = nn.ReLU\n        elif activation == \"tanh\":\n            self.activation = nn.Tanh\n        elif activation == \"sigmoid\":\n            self.activation = nn.Sigmoid\n        else:\n            raise RuntimeError(\"Activation not found.\")\n\n        self.learning_rate = learning_rate\n        self.dropout_rate = dropout_rate\n        self.batch_size = batch_size\n\n        self.data_dir = os.path.join(os.getcwd(), \"datasets\")\n        self.num_classes = 10\n        self.dims = (1, 28, 28)\n        self.channels, self.width, self.height = self.dims\n        self.transform = transforms.Compose(\n            [\n                transforms.ToTensor(),\n                transforms.Normalize((0.1307,), (0.3081,)),\n            ]\n        )\n\n        self.accuracy = Accuracy(task=\"multiclass\", num_classes=self.num_classes)\n\n    def prepare_data(self):\n        # download\n        MNIST(self.data_dir, train=True, download=True)\n        MNIST(self.data_dir, train=False, download=True)\n\n    def setup(self, stage=None):\n        # Assign train/val datasets for use in dataloaders\n        if stage == \"fit\" or stage is None:\n            mnist_full = MNIST(self.data_dir, train=True, transform=self.transform)\n            self.mnist_train, self.mnist_val = random_split(mnist_full, [20000, 40000])\n\n        # Assign test dataset for use in dataloader(s)\n        if stage == \"test\" or stage is None:\n            self.mnist_test = MNIST(self.data_dir, train=False, transform=self.transform)\n\n    def train_dataloader(self):\n        return DataLoader(self.mnist_train, batch_size=self.batch_size, num_workers=NUM_WORKERS)\n\n    def val_dataloader(self):\n        return DataLoader(self.mnist_val, batch_size=self.batch_size, num_workers=NUM_WORKERS)\n\n    def test_dataloader(self):\n        return DataLoader(self.mnist_test, batch_size=self.batch_size, num_workers=NUM_WORKERS)\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = F.nll_loss(logits, y)\n\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        logits = self(x)\n        loss = F.nll_loss(logits, y)\n        preds = torch.argmax(logits, dim=1)\n        self.accuracy(preds, y)\n\n        self.log(\"val_loss\", loss, prog_bar=True)\n        self.log(\"val_acc\", self.accuracy, prog_bar=True)\n\n        return loss\n\n    def test_step(self, batch, batch_idx):\n        return self.validation_step(batch, batch_idx)\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n        return optimizer\n\n\nclass MLP(MNISTModel):\n    def __init__(self, activation, learning_rate, dropout_rate, batch_size, num_neurons=(64, 32)):\n        super().__init__(activation, learning_rate, dropout_rate, batch_size)\n\n        self.layers = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(self.channels * self.width * self.height, num_neurons[0]),\n            self.activation(),\n            nn.Dropout(dropout_rate),\n            nn.Linear(num_neurons[0], num_neurons[1]),\n            self.activation(),\n            nn.Dropout(dropout_rate),\n            nn.Linear(num_neurons[1], self.num_classes),\n        )\n\n    def forward(self, x):\n        x = self.layers(x)\n        return F.log_softmax(x, dim=1)\n\n\nclass CNN(MNISTModel):\n    def __init__(self, activation, learning_rate, dropout_rate, batch_size):\n        super().__init__(activation, learning_rate, dropout_rate, batch_size)\n\n        self.conv1 = nn.Sequential(\n            nn.Conv2d(\n                in_channels=self.channels,\n                out_channels=16,\n                kernel_size=5,\n                stride=1,\n                padding=2,\n            ),\n            self.activation(),\n            nn.Dropout(dropout_rate),\n            nn.MaxPool2d(kernel_size=2),\n        )\n        self.conv2 = nn.Sequential(\n            nn.Conv2d(16, 32, 5, 1, 2),\n            self.activation(),\n            nn.Dropout(dropout_rate),\n            nn.MaxPool2d(2),\n            nn.Flatten(),\n        )\n        # fully connected layer, output 10 classes\n        self.out = nn.Linear(32 * 7 * 7, self.num_classes)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = self.out(x)\n\n        return F.log_softmax(x, dim=1)\n\n\ndef get_configspace(seed):\n    configspace = ConfigurationSpace(seed=seed)\n\n    model = CategoricalHyperparameter(name=\"model\", choices=[\"mlp\", \"cnn\"])\n    activation = CategoricalHyperparameter(name=\"activation\", choices=[\"sigmoid\", \"tanh\", \"relu\"])\n    learning_rate = UniformFloatHyperparameter(\n        name=\"learning_rate\", lower=0.0001, upper=0.1, log=True\n    )\n    dropout_rate = UniformFloatHyperparameter(name=\"dropout_rate\", lower=0.1, upper=0.9)\n    batch_size = UniformIntegerHyperparameter(name=\"batch_size\", lower=16, upper=256)\n\n    # MLP specific\n    num_neurons_layer1 = UniformIntegerHyperparameter(name=\"num_neurons_layer1\", lower=5, upper=100)\n    num_neurons_layer2 = UniformIntegerHyperparameter(name=\"num_neurons_layer2\", lower=5, upper=100)\n\n    configspace.add_hyperparameters(\n        [\n            model,\n            activation,\n            learning_rate,\n            dropout_rate,\n            batch_size,\n            num_neurons_layer1,\n            num_neurons_layer2,\n        ]\n    )\n\n    # Now add sub configspace\n    configspace.add_condition(CS.EqualsCondition(num_neurons_layer1, model, \"mlp\"))\n    configspace.add_condition(CS.EqualsCondition(num_neurons_layer2, model, \"mlp\"))\n\n    return configspace\n\n\nif __name__ == \"__main__\":\n    # Define objectives\n    accuracy = Objective(\"accuracy\", lower=0, upper=1, optimize=\"upper\")\n    loss = Objective(\"loss\", lower=0, optimize=\"lower\")\n    time = Objective(\"time\", lower=0, optimize=\"lower\")\n\n    # Define budgets\n    max_epochs = 8\n    n_epochs = 4\n    budgets = np.linspace(0, max_epochs, num=n_epochs)\n\n    # Others\n    num_configs = 1000\n    num_runs = 3\n    save_path = \"logs/DeepCAVE/mnist_pytorch\"\n\n    for run_id in range(num_runs):\n        random.seed(run_id)\n        configspace = get_configspace(run_id)\n\n        with Recorder(configspace, objectives=[accuracy, loss, time], save_path=save_path) as r:\n            for config in configspace.sample_configuration(num_configs):\n                pl.seed_everything(run_id)\n                kwargs = dict(\n                    activation=config[\"activation\"],\n                    learning_rate=config[\"learning_rate\"],\n                    dropout_rate=config[\"dropout_rate\"],\n                    batch_size=config[\"batch_size\"],\n                )\n\n                if config[\"model\"] == \"mlp\":\n                    model = MLP(\n                        **kwargs,\n                        num_neurons=(\n                            config[\"num_neurons_layer1\"],\n                            config[\"num_neurons_layer2\"],\n                        ),\n                    )\n                elif config[\"model\"] == \"cnn\":\n                    model = CNN(**kwargs)  # type: ignore\n\n                start_time = t.time()\n                for i in range(1, n_epochs):\n                    budget = budgets[i]\n                    # How many epochs has to be run in this round\n                    epochs = int(budgets[i]) - int(budgets[i - 1])\n\n                    pl.seed_everything(run_id)\n                    r.start(config, budget, model=model)\n\n                    # The model weights are trained\n                    trainer = pl.Trainer(\n                        accelerator=\"cpu\",\n                        devices=1,\n                        num_sanity_val_steps=0,  # No validation sanity\n                        deterministic=True,\n                        min_epochs=epochs,\n                        max_epochs=epochs,\n                    )\n                    trainer.fit(model)\n                    result = trainer.test(model)\n                    accuracy_ = result[0][\"val_acc\"]\n                    loss_ = result[0][\"val_loss\"]\n\n                    # We just add some random stati to show the potential later in DeepCAVE\n                    if accuracy_ < 0.5:\n                        status = Status.CRASHED\n                        accuracy_, loss_ = None, None\n                    elif random.uniform(0, 1) < 0.05:  # 5% chance\n                        statusses = [Status.MEMORYOUT, Status.TIMEOUT]\n                        status = random.choice(statusses)\n                        accuracy_, loss_ = None, None\n                    else:\n                        status = Status.SUCCESS\n\n                    end_time = t.time()\n                    elapsed_time = end_time - start_time\n\n                    r.end(costs=[accuracy_, loss_, elapsed_time], status=status)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}