{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Multi-Layer Perceptron via Sklearn\n\nThis more advanced example shows how sklearn can be used to record an optimization\nprocess in DeepCAVE format.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import warnings\n\nfrom sklearn.exceptions import ConvergenceWarning\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.model_selection import train_test_split\nfrom ConfigSpace import ConfigurationSpace\nfrom ConfigSpace.hyperparameters import (\n    UniformFloatHyperparameter,\n    CategoricalHyperparameter,\n    UniformIntegerHyperparameter,\n)\nfrom deepcave import Recorder, Objective\nfrom sklearn.datasets import load_digits\n\nfrom deepcave.utils.util import print_progress_bar\n\n\ndef get_dataset():\n    digits = load_digits()\n\n    X, y = digits.data, digits.target\n    X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=0)\n\n    return X_train, X_test, y_train, y_test\n\n\ndef get_configspace(seed):\n    configspace = ConfigurationSpace(seed=seed)\n    num_neurons_layer1 = UniformIntegerHyperparameter(name=\"num_neurons_layer1\", lower=5, upper=100)\n    num_neurons_layer2 = UniformIntegerHyperparameter(name=\"num_neurons_layer2\", lower=5, upper=100)\n    activation = CategoricalHyperparameter(name=\"activation\", choices=[\"logistic\", \"tanh\", \"relu\"])\n    solver = CategoricalHyperparameter(name=\"solver\", choices=[\"sgd\", \"adam\"])\n    batch_size = UniformIntegerHyperparameter(name=\"batch_size\", lower=1, upper=100)\n    learning_rate = UniformFloatHyperparameter(\n        name=\"learning_rate\", lower=0.0001, upper=0.1, log=True\n    )\n\n    configspace.add(\n        [\n            num_neurons_layer1,\n            num_neurons_layer2,\n            activation,\n            solver,\n            batch_size,\n            learning_rate,\n        ]\n    )\n\n    return configspace\n\n\ndef progress_bar(iterable, prefix=\"\", suffix=\"\", decimals=1, length=100, fill=\"\u2588\", printEnd=\"\\r\"):\n    \"\"\"\n    Call in a loop to create terminal progress bar\n    @params:\n        iterable    - Required  : iterable object (Iterable)\n        prefix      - Optional  : prefix string (Str)\n        suffix      - Optional  : suffix string (Str)\n        decimals    - Optional  : positive number of decimals in percent complete (Int)\n        length      - Optional  : character length of bar (Int)\n        fill        - Optional  : bar fill character (Str)\n        printEnd    - Optional  : end character (e.g. \"\\r\", \"\\r\\n\") (Str)\n    \"\"\"\n    total = len(iterable)\n\n    # Progress Bar Printing Function\n\n    # Initial Call\n    print_progress_bar(0)\n    # Update Progress Bar\n    for i, item in enumerate(iterable):\n        yield item\n        print_progress_bar(i + 1)\n    # Print New Line on Complete\n    print()\n\n\nif __name__ == \"__main__\":\n    warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n\n    # Get dataset\n    X_train, X_test, y_train, y_test = get_dataset()\n\n    # Define objectives\n    accuracy = Objective(\"accuracy\", lower=0, upper=1, optimize=\"upper\")\n    time = Objective(\"time\", lower=0, optimize=\"lower\")\n\n    # Define budgets\n    budgets = [10, 30, 90]\n\n    # Others\n    num_configs = 20\n    save_path = \"logs/DeepCAVE/digits_sklearn\"\n    seed = 42\n\n    configspace = get_configspace(seed)\n\n    with Recorder(configspace, objectives=[accuracy, time], save_path=save_path) as r:\n        configs = configspace.sample_configuration(num_configs)\n        print_progress_bar(num_configs, 0)\n        for config_i in range(len(configs)):\n            config = configs[config_i]\n\n            for budget in budgets:\n                r.start(config, budget)\n                clf = MLPClassifier(\n                    random_state=seed,\n                    max_iter=budget,\n                    hidden_layer_sizes=(\n                        config[\"num_neurons_layer1\"],\n                        config[\"num_neurons_layer2\"],\n                    ),\n                    activation=config[\"activation\"],\n                    solver=config[\"solver\"],\n                    batch_size=config[\"batch_size\"],\n                    learning_rate_init=config[\"learning_rate\"],\n                )\n                clf.fit(X_train, y_train)\n                score = clf.score(X_test, y_test)\n\n                r.end(costs=[score, None], seed=seed)\n\n            # print(f\"Config {config_i + 1}/{num_configs}\")\n            print_progress_bar(num_configs, config_i + 1, prefix=\"Training Progress\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.20"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}