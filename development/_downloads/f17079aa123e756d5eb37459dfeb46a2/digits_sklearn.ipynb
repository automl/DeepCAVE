{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Multi-Layer Perceptron via Sklearn\n\nThis more advanced example shows how sklearn can be used to record an optimization\nprocess in DeepCAVE format.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.neural_network import MLPClassifier\nfrom sklearn.model_selection import train_test_split\nfrom ConfigSpace import ConfigurationSpace\nfrom ConfigSpace.hyperparameters import (\n    UniformFloatHyperparameter,\n    CategoricalHyperparameter,\n    UniformIntegerHyperparameter,\n)\nfrom deepcave import Recorder, Objective\nfrom sklearn.datasets import load_digits\n\n\ndef get_dataset():\n    digits = load_digits()\n\n    X, y = digits.data, digits.target\n    X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=0)\n\n    return X_train, X_test, y_train, y_test\n\n\ndef get_configspace(seed):\n    configspace = ConfigurationSpace(seed=seed)\n    num_neurons_layer1 = UniformIntegerHyperparameter(name=\"num_neurons_layer1\", lower=5, upper=100)\n    num_neurons_layer2 = UniformIntegerHyperparameter(name=\"num_neurons_layer2\", lower=5, upper=100)\n    activation = CategoricalHyperparameter(name=\"activation\", choices=[\"logistic\", \"tanh\", \"relu\"])\n    solver = CategoricalHyperparameter(name=\"solver\", choices=[\"sgd\", \"adam\"])\n    batch_size = UniformIntegerHyperparameter(name=\"batch_size\", lower=1, upper=100)\n    learning_rate = UniformFloatHyperparameter(\n        name=\"learning_rate\", lower=0.0001, upper=0.1, log=True\n    )\n\n    configspace.add_hyperparameters(\n        [\n            num_neurons_layer1,\n            num_neurons_layer2,\n            activation,\n            solver,\n            batch_size,\n            learning_rate,\n        ]\n    )\n\n    return configspace\n\n\nif __name__ == \"__main__\":\n    # Get dataset\n    X_train, X_test, y_train, y_test = get_dataset()\n\n    # Define objectives\n    accuracy = Objective(\"accuracy\", lower=0, upper=1, optimize=\"upper\")\n    time = Objective(\"time\", lower=0, optimize=\"lower\")\n\n    # Define budgets\n    budgets = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n\n    # Others\n    num_configs = 200\n    num_runs = 5\n    save_path = \"logs/DeepCAVE/digits_sklearn\"\n\n    for run_id in range(num_runs):\n        configspace = get_configspace(run_id)\n\n        with Recorder(configspace, objectives=[accuracy, time], save_path=save_path) as r:\n            for config in configspace.sample_configuration(num_configs):\n                for budget in budgets:\n                    r.start(config, budget)\n                    clf = MLPClassifier(\n                        random_state=run_id,\n                        max_iter=budget,\n                        hidden_layer_sizes=(\n                            config[\"num_neurons_layer1\"],\n                            config[\"num_neurons_layer2\"],\n                        ),\n                        activation=config[\"activation\"],\n                        solver=config[\"solver\"],\n                        batch_size=config[\"batch_size\"],\n                        learning_rate_init=config[\"learning_rate\"],\n                    )\n                    clf.fit(X_train, y_train)\n                    score = clf.score(X_test, y_test)\n\n                    r.end(costs=[score, None])"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}